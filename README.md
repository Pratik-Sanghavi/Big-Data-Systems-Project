# **ADAP**ter: Power Efficient Model Inference on Nvidia GPUs
**A**nte Tonkovic-Capin, **D**rishan Poovaya, **A**shu Kumar, **P**ratik Sanghavi

---

## Repository Structure
This repository is part of a broader submission for CS 744 - Big Data Systems for Group 11. The primary submission is the final research report. This repository is any related or supplemental material that we thought worthwhile including as additional source. The repository is structured:

```text
Project
   ├── README.md
   ├── data
   │   └── imagenet_payload.json
   ├── direct_inference
   │   ├── DeepOnxxInference
   │   │   ├── createPayload.py
   │   │   ├── deepOnxxScript.py
   │   │   ├── onxxPython.py
   │   │   ├── onxx_inference.sh
   │   │   ├── payload.json
   │   │   └── serializedImage.txt
   │   ├── repeat-inference.sh
   │   ├── tegra-helper.sh
   │   └── tegrastats-tgparse.sh
   ├── docs
   │   ├── ADAPter - Final Report.pdf
   │   ├── ADAPter - Poster.pdf
   │   ├── ADAPter - Poster.pptx
   │   ├── Project Check-In.pdf
   │   └── Project Introduction.pdf
   ├── power_utils
   │   └── monitor_metrics_tegra.c
   ├── queue_server.py
   ├── queue_server_baseline.py
   ├── send_request.py
   ├── send_request_gpt.py
   └── tgparse.py

5 directories, 22 files
```

---

## Primary System
The majority of this project's work was done on a UW-Madison machine, `128.105.102.4`, running Ubunutu 18.04:

```text
   Static hostname: nvidia-desktop
         Icon name: computer
  Operating System: Ubuntu 18.04.6 LTS
            Kernel: Linux 4.9.299-tegra
      Architecture: arm64
```
The system contained a very specific set of dependencies. Therefore the majority of the source code for this project may not be directly transferrable or usable across a broader set of host systems or devices. Those that may be useable or transferrable are detailed in the following sections. Special thank you to Minghao Yan for allowing us the use of his system and device, we hope we kept our part of the bargain and didn't change anything that would cause issues for you after we finished.

---

## ADAPter
This is the system we introduced as part of our work in power-efficient inference. The design is already described in the report. Here we'll show how to run the `ADAPter`.
- First add the models to the model repository of triton server and add the config file for it. Use netron to visualize the weights file and describe the file format and inputs and outputs of the model in the config file. Refer other `config.pbtxt` files to know the fields to include. We recommend using the trt backend (convert `onnx` models to `plan` format) with quantization since that will really speed up inference and get you the best performance on the Jetson device.
- Run the triton inference server using `tritonserver --model-repository=<PATH TO MODEL REPO> --backend-directory=<PATH TO BACKEND> --backend-config=tensorrt,version=8 --allow-gpu-metrics true`
- Modify the endpoint based on which you plan to inference with. Change the constants based on how much smoothing is desirable. More smoothing will allow `ADAPter` to settle better but react slowly to sudden changes. Currently we support only one endpoint (this is by design since we want a homogenous stream of jobs) but its simple enough to add different endpoints on different listener threads with a few code tweaks. Start the queue server aka `ADAPter` with `sudo python queue_server.py`.
- Tweak `send_requests.py` to send preprocessed payload data to `ADAPter` based on the endpoint you'll be hitting.
- Use the tegrastats utility for monitoring system stats.

---

## Tegrastats Parsing
We wrote a python utility, `tgparse.py` for parsing the GPU usage data in the log file generated by the `sudo tegrastats` command which can be used by:

```bash
$ python tgparse.py [-h] [-o OUTPUT_FILE] [-i INTERVAL] [-q] log_file
```

We also wrote a bash script, `tegrastats-parse.sh`, that wraps `tgparse.py` and the `sudo tegrastats` command for running, generating and parsing the stats by: 

```bash
$ ./tegrastats-tgparse.sh <interval_in_ms> <count> <output>
```

Which runs the tegrastats utility for the specified count with the interval provided and generates the parsed csv to the output provided. Special shoutout to [SQLDataModel](https://pypi.org/project/SQLDataModel/) for their lightweight and performant library that helped us write the parser! It really was a godsend.

---

## Contributions
While all the members of ADAP's 11 feel everyone contributed in good faith, per the requirement for specific contributions, see below:

**Ante Tonkovic-Capin**
- Provided emotional support to rest of the group when times were hard
- System config, network and proxy configuration for project dependencies, parsing documentation
- Created sections of Poster
- Wrote sections of Final Report
- Code and Process Documentation
- `tgparse.py`
- `tegrastats-tgparse.sh`
- Parsing data and generating plots

**Drishan Poovaya**
- Provided emotional support to rest of the group when times were hard
- System config, network and proxy configuration for project dependencies, parsing documentation
- Created sections of Poster
- Wrote sections of Final Report

**Ashu Kumar**
- Provided emotional support to rest of the group when times were hard
- System config, network and proxy configuration for project dependencies, parsing documentation
- Created sections of Poster
- Wrote sections of Final Report

**Pratik Sanghavi**
- Provided emotional support to rest of the group when times were hard
- System config, network and proxy configuration for project dependencies, parsing documentation
- Created sections of Poster
- Wrote sections of Final Report

---

## Final Note
Have questions or comments regarding what you see? Don't hesitate to reach out!

**Thank you!**
__- ADAP's 11__
